{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4264fc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Iterable, Optional, Set\n",
    "# ===== Ruta B (LDA/TF-IDF): tokens lematizados y sin stopwords =====\n",
    "import sys, spacy\n",
    "from spacy.cli import download\n",
    "download(\"es_core_news_sm\")\n",
    "nlp = spacy.load(\"es_core_news_sm\", disable=[\"ner\",\"parser\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cb093f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "def load_jsonl_to_df(path_jsonl: str) -> pd.DataFrame:\n",
    "    df = pd.read_json(path_jsonl, lines=True)\n",
    "    if \"fecha\" in df.columns:\n",
    "        df[\"fecha\"] = pd.to_datetime(df[\"fecha\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def structural_cleaning(df: pd.DataFrame, keep_cols: Optional[Iterable[str]] = None) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if keep_cols is not None:\n",
    "        keep_cols = [c for c in df.columns if c in keep_cols]\n",
    "        df = df[keep_cols].copy()\n",
    "    if \"identificador\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"identificador\"])\n",
    "    if \"texto_limpio\" in df.columns:\n",
    "        df = df.drop_duplicates(subset=[\"texto_limpio\"])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def empty_to_nan(df, cols=None, extra_empty_tokens=None):\n",
    "    df = df.copy()\n",
    "    if cols is None:\n",
    "        cols = df.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
    "\n",
    "    # Tokens a tratar como vacíos\n",
    "    tokens = {\"\", \" \", \"null\", \"NULL\", \"None\", \"none\", \"N/A\", \"NA\", \"—\", \"-\", \"–\"}\n",
    "    if extra_empty_tokens:\n",
    "        tokens |= set(extra_empty_tokens)\n",
    "\n",
    "    # strip() y reemplazo por NaN\n",
    "    for c in cols:\n",
    "        df[c] = df[c].astype(\"string\")  # preserva NaNs y permite .str ops\n",
    "        df[c] = df[c].str.strip()\n",
    "        df[c] = df[c].replace(list(tokens), np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_missingness_then_drop_small(\n",
    "    df: pd.DataFrame, threshold_pct: float = 1.0, critical_cols=None, normalize_empty=True, extra_empty_tokens=None):\n",
    "    if normalize_empty:\n",
    "        df = empty_to_nan(df, extra_empty_tokens=extra_empty_tokens)\n",
    "\n",
    "    total_rows = max(len(df), 1)\n",
    "    missing_pct = (df.isna().sum() / total_rows * 100).round(2).sort_values(ascending=False)\n",
    "\n",
    "    # 1) Quita filas con nulos en columnas críticas\n",
    "    if critical_cols:\n",
    "        df = df.dropna(subset=list(critical_cols))\n",
    "\n",
    "    # 2) Para columnas con pocos nulos, elimina esas filas nulas\n",
    "    low_na_cols = missing_pct[missing_pct < threshold_pct].index.tolist()\n",
    "    for c in low_na_cols:\n",
    "        df = df[df[c].notna()]\n",
    "\n",
    "    # Recalcular reporte tras los drops\n",
    "    total_rows2 = max(len(df), 1)\n",
    "    missing_pct_after = (df.isna().sum() / total_rows2 * 100).round(2).sort_values(ascending=False)\n",
    "\n",
    "    return df, missing_pct_after\n",
    "\n",
    "\n",
    "def _pop_last_df_ref() -> Optional[pd.DataFrame]:\n",
    "    return globals().pop(\"_last_df_ref\", None)\n",
    "\n",
    "\n",
    "def fill_missing_logical(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"fecha\" in df.columns and \"mes\" in df.columns:\n",
    "        mask = df[\"mes\"].isna() & df[\"fecha\"].notna()\n",
    "        df.loc[mask, \"mes\"] = pd.to_datetime(df.loc[mask, \"fecha\"], errors=\"coerce\").dt.strftime(\"%Y-%m\")\n",
    "    if \"texto_limpio\" in df.columns:\n",
    "        df[\"texto_limpio\"] = df[\"texto_limpio\"].fillna(\"\")\n",
    "    if \"tematica\" in df.columns:\n",
    "        df[\"tematica\"] = df[\"tematica\"].fillna(\"Desconocido\")\n",
    "    return df\n",
    "\n",
    "def detect_inconsistencies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    issues = {}\n",
    "    if \"fecha\" in df.columns and \"mes\" in df.columns:\n",
    "        f = pd.to_datetime(df[\"fecha\"], errors=\"coerce\")\n",
    "        m_calc = f.dt.strftime(\"%Y-%m\")\n",
    "        issues[\"mes_mismatch\"] = (df[\"mes\"].notna()) & (m_calc.notna()) & (df[\"mes\"] != m_calc)\n",
    "    if \"trimestre\" in df.columns:\n",
    "        valid_trim = {\"Q1\",\"Q2\",\"Q3\",\"Q4\", None, np.nan}\n",
    "        issues[\"trimestre_invalid\"] = ~df[\"trimestre\"].isin(valid_trim)\n",
    "    if \"texto_limpio\" in df.columns:\n",
    "        issues[\"texto_muy_corto\"] = df[\"texto_limpio\"].str.len().fillna(0) < 20\n",
    "    inc = pd.DataFrame(issues) if issues else pd.DataFrame(index=df.index)\n",
    "    if not inc.empty:\n",
    "        inc[\"any_issue\"] = inc.any(axis=1)\n",
    "    return inc\n",
    "\n",
    "RE_URL = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "RE_HTML = re.compile(r'<[^>]+>')\n",
    "RE_EMAIL = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b')\n",
    "RE_PHONE = re.compile(r'(?:(?:\\+?\\d{1,3})?[\\s.-]?)?(?:\\(?\\d{2,3}\\)?[\\s.-]?)?\\d{3,4}[\\s.-]?\\d{3,4}')\n",
    "RE_NIF = re.compile(r'\\b[XYZ]?\\d{5,8}[A-Z]\\b')\n",
    "\n",
    "def _clean_text_minimal(s: str, strip_pii: bool = True) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    x = s\n",
    "    x = RE_URL.sub(\" \", x)\n",
    "    x = RE_HTML.sub(\" \", x)\n",
    "    if strip_pii:\n",
    "        x = RE_EMAIL.sub(\" \", x)\n",
    "        x = RE_PHONE.sub(\" \", x)\n",
    "        x = RE_NIF.sub(\" \", x)\n",
    "    x = unicodedata.normalize(\"NFKC\", x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x\n",
    "\n",
    "def minimal_text_clean(df: pd.DataFrame, text_col: str = \"texto_limpio\", strip_pii: bool = True) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if text_col in df.columns:\n",
    "        df[text_col] = df[text_col].map(lambda s: _clean_text_minimal(s, strip_pii=strip_pii))\n",
    "    return df\n",
    "\n",
    "def configure_spacy(extra_stop: Optional[Iterable[str]] = None):\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"es_core_news_sm\", disable=[\"ner\",\"parser\"])\n",
    "    STOP_ES = set(nlp.Defaults.stop_words)\n",
    "    if extra_stop:\n",
    "        STOP_ES |= set(extra_stop)\n",
    "    return nlp, STOP_ES\n",
    "\n",
    "def _expand_contractions_es(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = re.sub(r\"\\bdel\\b\", \"de el\", s)\n",
    "    s = re.sub(r\"\\bal\\b\", \"a el\", s)\n",
    "    return s\n",
    "\n",
    "def preprocess_for_bow(text: str, nlp, STOP_ES: Set[str]) -> list:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    t = _expand_contractions_es(text.lower())\n",
    "    doc = nlp(t)\n",
    "    toks = []\n",
    "    for w in doc:\n",
    "        if w.is_punct or w.is_space or w.is_digit:\n",
    "            continue\n",
    "        lemma = w.lemma_.strip()\n",
    "        if len(lemma) < 2:\n",
    "            continue\n",
    "        if lemma in STOP_ES:\n",
    "            continue\n",
    "        toks.append(lemma)\n",
    "    return toks\n",
    "\n",
    "def run_bow_pipeline(df: pd.DataFrame, nlp, STOP_ES: set, text_col: str = \"texto_limpio\", suffix: str = \"\") -> pd.DataFrame:\n",
    "    \n",
    "    df = df.copy()\n",
    "\n",
    "    # Procesamiento robusto: evita errores con valores NaN o no string\n",
    "    df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
    "\n",
    "    # Tokenización y limpieza\n",
    "    df[f\"tokens_{suffix or text_col}\"] = df[text_col].map(\n",
    "        lambda t: preprocess_for_bow(t, nlp=nlp, STOP_ES=STOP_ES)\n",
    "    )\n",
    "    df[f\"n_tokens_{suffix or text_col}\"] = df[f\"tokens_{suffix or text_col}\"].map(len)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_outputs(\n",
    "    df: pd.DataFrame,\n",
    "    path_parquet: str,\n",
    "    path_missing_csv: Optional[str] = None,\n",
    "    path_inconsistencies_csv: Optional[str] = None,\n",
    "    inconsistencies_df: Optional[pd.DataFrame] = None\n",
    "):\n",
    "    os.makedirs(os.path.dirname(path_parquet) or \".\", exist_ok=True)\n",
    "    df.to_parquet(path_parquet, index=False)\n",
    "    if path_missing_csv:\n",
    "        os.makedirs(os.path.dirname(path_missing_csv) or \".\", exist_ok=True)\n",
    "        na_pct = (df.isna().sum() / max(len(df),1) * 100).round(2)\n",
    "        na_pct.to_csv(path_missing_csv, header=[\"pct_missing\"])\n",
    "    if path_inconsistencies_csv and inconsistencies_df is not None and not inconsistencies_df.empty:\n",
    "        os.makedirs(os.path.dirname(path_inconsistencies_csv) or \".\", exist_ok=True\n",
    "        )\n",
    "        inconsistencies_df.to_csv(path_inconsistencies_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6203f9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identificador</th>\n",
       "      <th>fecha</th>\n",
       "      <th>diario_numero</th>\n",
       "      <th>seccion_codigo</th>\n",
       "      <th>seccion_nombre</th>\n",
       "      <th>departamento_nombre</th>\n",
       "      <th>epigrafe_nombre</th>\n",
       "      <th>titulo</th>\n",
       "      <th>tematica</th>\n",
       "      <th>texto_limpio</th>\n",
       "      <th>mes</th>\n",
       "      <th>trimestre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BOE-A-2024-1</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2A</td>\n",
       "      <td>II. Autoridades y personal. - A. Nombramientos...</td>\n",
       "      <td>MINISTERIO DE TRABAJO Y ECONOMÍA SOCIAL</td>\n",
       "      <td>Destinos</td>\n",
       "      <td>Resolución de 21 de diciembre de 2023, de la S...</td>\n",
       "      <td>Otras</td>\n",
       "      <td>Por Resolución de la Subsecretaría de este Dep...</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOE-A-2024-2</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2A</td>\n",
       "      <td>II. Autoridades y personal. - A. Nombramientos...</td>\n",
       "      <td>UNIVERSIDADES</td>\n",
       "      <td>Nombramientos</td>\n",
       "      <td>Resolución de 21 de diciembre de 2023, de la U...</td>\n",
       "      <td>Economía/Empresa</td>\n",
       "      <td>Vistas las propuestas elevadas por las comisio...</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BOE-A-2024-3</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2A</td>\n",
       "      <td>II. Autoridades y personal. - A. Nombramientos...</td>\n",
       "      <td>UNIVERSIDADES</td>\n",
       "      <td>Nombramientos</td>\n",
       "      <td>Resolución de 22 de diciembre de 2023, conjunt...</td>\n",
       "      <td>Sanidad</td>\n",
       "      <td>Vista la propuesta elevada el 18 de diciembre ...</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BOE-A-2024-4</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2A</td>\n",
       "      <td>II. Autoridades y personal. - A. Nombramientos...</td>\n",
       "      <td>UNIVERSIDADES</td>\n",
       "      <td>Nombramientos</td>\n",
       "      <td>Resolución de 22 de diciembre de 2023, de la U...</td>\n",
       "      <td>Economía/Empresa</td>\n",
       "      <td>De conformidad con la propuesta elevada por la...</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOE-A-2024-5</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2A</td>\n",
       "      <td>II. Autoridades y personal. - A. Nombramientos...</td>\n",
       "      <td>UNIVERSIDADES</td>\n",
       "      <td>Nombramientos</td>\n",
       "      <td>Resolución de 22 de diciembre de 2023, de la U...</td>\n",
       "      <td>Educación/Universidad</td>\n",
       "      <td>De conformidad con la propuesta elevada por la...</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>Q1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  identificador      fecha diario_numero seccion_codigo  \\\n",
       "0  BOE-A-2024-1 2024-01-01             1             2A   \n",
       "1  BOE-A-2024-2 2024-01-01             1             2A   \n",
       "2  BOE-A-2024-3 2024-01-01             1             2A   \n",
       "3  BOE-A-2024-4 2024-01-01             1             2A   \n",
       "4  BOE-A-2024-5 2024-01-01             1             2A   \n",
       "\n",
       "                                      seccion_nombre  \\\n",
       "0  II. Autoridades y personal. - A. Nombramientos...   \n",
       "1  II. Autoridades y personal. - A. Nombramientos...   \n",
       "2  II. Autoridades y personal. - A. Nombramientos...   \n",
       "3  II. Autoridades y personal. - A. Nombramientos...   \n",
       "4  II. Autoridades y personal. - A. Nombramientos...   \n",
       "\n",
       "                       departamento_nombre epigrafe_nombre  \\\n",
       "0  MINISTERIO DE TRABAJO Y ECONOMÍA SOCIAL        Destinos   \n",
       "1                            UNIVERSIDADES   Nombramientos   \n",
       "2                            UNIVERSIDADES   Nombramientos   \n",
       "3                            UNIVERSIDADES   Nombramientos   \n",
       "4                            UNIVERSIDADES   Nombramientos   \n",
       "\n",
       "                                              titulo               tematica  \\\n",
       "0  Resolución de 21 de diciembre de 2023, de la S...                  Otras   \n",
       "1  Resolución de 21 de diciembre de 2023, de la U...       Economía/Empresa   \n",
       "2  Resolución de 22 de diciembre de 2023, conjunt...                Sanidad   \n",
       "3  Resolución de 22 de diciembre de 2023, de la U...       Economía/Empresa   \n",
       "4  Resolución de 22 de diciembre de 2023, de la U...  Educación/Universidad   \n",
       "\n",
       "                                        texto_limpio      mes trimestre  \n",
       "0  Por Resolución de la Subsecretaría de este Dep...  2024-01        Q1  \n",
       "1  Vistas las propuestas elevadas por las comisio...  2024-01        Q1  \n",
       "2  Vista la propuesta elevada el 18 de diciembre ...  2024-01        Q1  \n",
       "3  De conformidad con la propuesta elevada por la...  2024-01        Q1  \n",
       "4  De conformidad con la propuesta elevada por la...  2024-01        Q1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# 1) Cargar\n",
    "dataset = load_dataset(\"Joz16gg162/boe_2024_dataset\", split=\"full\")\n",
    "df = dataset.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "322983ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Limpieza estructural\n",
    "KEEP = {\n",
    "    \"identificador\",\"fecha\",\"mes\",\"seccion_nombre\",\n",
    "    \"departamento_nombre\",\"epigrafe_nombre\",\"titulo\",\"texto_limpio\"\n",
    "}\n",
    "df = structural_cleaning(df, keep_cols=KEEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b79eed26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75382, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4804291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Analizar faltantes y eliminar filas con ausencias pequeñas\n",
    "df, missing_report = analyze_missingness_then_drop_small(\n",
    "    df,\n",
    "    threshold_pct=1.0,\n",
    "    critical_cols=[\"identificador\", \"texto_limpio\", \"epigrafe_nombre\"],\n",
    "    normalize_empty=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e884126d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27622, 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a13ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Rellenar lo que sí tiene sentido (mes desde fecha, tematica vacía -> \"Desconocido\", etc.)\n",
    "df = fill_missing_logical(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6be0bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Inconsistencias (mes vs fecha, trimestre válido, texto corto)\n",
    "inc = detect_inconsistencies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f97df4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Limpieza mínima de texto (URLs/HTML/PII + espacios)\n",
    "df = minimal_text_clean(df, text_col=\"texto_limpio\", strip_pii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7c1ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Ruta A (Transformers/LLMs): mantiene texto natural =====\n",
    "save_outputs(\n",
    "    df,\n",
    "    path_parquet=\"data/boe_2024_clean.parquet\",\n",
    "    path_missing_csv=\"data/missing_report.csv\",\n",
    "    path_inconsistencies_csv=\"data/inconsistencies_report.csv\",\n",
    "    inconsistencies_df=inc\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a85b79f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nlp, STOP_ES = configure_spacy(extra_stop={\\n    \"artículo\",\"disposición\",\"resuelve\",\"boe\",\"anexo\",\"rector\",\\n    \"subsecretaría\",\"tribunal\",\"resolución\",\"convocatoria\",\"administrativo\"\\n})\\n# Tokenizar solo texto_limpio\\ndf_texto = run_bow_pipeline(df, nlp=nlp, STOP_ES=STOP_ES, text_col=\"texto_limpio\", suffix=\"texto\")\\n\\n# Tokenizar solo titulo\\ndf_titulo = run_bow_pipeline(df, nlp=nlp, STOP_ES=STOP_ES, text_col=\"titulo\", suffix=\"titulo\")\\n\\n# Unir resultados\\ndf_bow = df[[\"identificador\", \"titulo\", \"texto_limpio\"]]     .merge(df_texto[[\"identificador\", \"tokens_texto\"]], on=\"identificador\", how=\"left\")     .merge(df_titulo[[\"identificador\", \"tokens_titulo\"]], on=\"identificador\", how=\"left\")\\n\\n# Guardar\\nsave_outputs(df_bow, path_parquet=\"data/processed/boe_2024_tokens_titulo_texto.parquet\")\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''nlp, STOP_ES = configure_spacy(extra_stop={\n",
    "    \"artículo\",\"disposición\",\"resuelve\",\"boe\",\"anexo\",\"rector\",\n",
    "    \"subsecretaría\",\"tribunal\",\"resolución\",\"convocatoria\",\"administrativo\"\n",
    "})\n",
    "# Tokenizar solo texto_limpio\n",
    "df_texto = run_bow_pipeline(df, nlp=nlp, STOP_ES=STOP_ES, text_col=\"texto_limpio\", suffix=\"texto\")\n",
    "\n",
    "# Tokenizar solo titulo\n",
    "df_titulo = run_bow_pipeline(df, nlp=nlp, STOP_ES=STOP_ES, text_col=\"titulo\", suffix=\"titulo\")\n",
    "\n",
    "# Unir resultados\n",
    "df_bow = df[[\"identificador\", \"titulo\", \"texto_limpio\"]] \\\n",
    "    .merge(df_texto[[\"identificador\", \"tokens_texto\"]], on=\"identificador\", how=\"left\") \\\n",
    "    .merge(df_titulo[[\"identificador\", \"tokens_titulo\"]], on=\"identificador\", how=\"left\")\n",
    "\n",
    "# Guardar\n",
    "save_outputs(df_bow, path_parquet=\"data/processed/boe_2024_tokens_titulo_texto.parquet\")\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
